---
title: "Estimating a piecewise-constant signal on the 2d lattice"
output: rmarkdown::github_document
date: '2025-05-07'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
In this example, we show how to use `flexBART::flexBART` to estimate a piecewise-constant signal on a 2d lattice.
To really highlight the ability of **flexBART** to incorporate network-structure, in this example we will not have any covariates.
That is, at vertex $i$ in the network, we will observe $T$ noisy realizations of a constant $\mu_{i}$: $y_{it} \sim \mathcal{N}(\mu_{i}, 1)$ for $t = 1, \ldots, T.$ 
Our goal will be to recover the vector $(\mu_{1}, \ldots, \mu_{n}).$

In this experiment, we will hold out all data from 10% of the vertices.
The idea is to see how well `flexBART::networkBART` is able to predict $\mu_{i}$ at these vertices.
Note that when we train our model, we will give it the full network. In this way, our prediction task can be viewed as ``in-fill'' rather predicting at a previously unobserved vertex in the network.


## Data Generation

We will start by creating our network, separating the vertices into five clusters, and setting the value of $\mu_{i}$ in each cluster.

The row and columns of the adjacency graph must be named and the names should be the levels of the categorical variable encoding vertex label. 
In this case, we'll use the integers 0 to (n-1)
```{r data_generation}
library(igraph)
n_side <- 10
n <- 100

g <- make_lattice(length = n_side, dim = 2)
A <- as_adjacency_matrix(g, type = "both", sparse = FALSE)
colnames(A) <- 0:(n-1)
rownames(A) <- 0:(n-1)

# make up 5 clusters
cluster1 <- 81:100
cluster1 <- cluster1[!cluster1 %in% c(89,90, 100)]
cluster2 <- c(89, 90, 100, 79, 80, 69, 70, 59, 60, 50, 40)
cluster3 <- rep(1:4, times = 4) + 10 * rep(0:3, each = 4) # lower 4x4 grid
cluster4 <- c(52, 53,63,54, 64, 55,65,56, 66,46, 
              47, 57, 67, 37, 38, 48, 28, 29, 27, 
              17, 16, 18, 6, 7, 8,30, 20, 10, 9, 19, 39, 49)
cluster5 <- which(!1:100 %in% c(cluster1, cluster2, cluster3, cluster4))

cluster_means <- c(7.5, 3, -2, -5, 0)
mu <- rep(NA, times = n)
mu[cluster1] <- cluster_means[1]
mu[cluster2] <- cluster_means[2]
mu[cluster3] <- cluster_means[3]
mu[cluster4] <- cluster_means[4]
mu[cluster5] <- cluster_means[5]
```

Here is a plot of the $\mu_{i}$'s.

```{r plot_mu, fig.align='center', fig.height=5, fig.width=5}
col_list <- colorBlindness::Blue2DarkRed18Steps
my_colors <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
g_true <- g
scaled_mu <- scales::rescale(mu, to = c(0,1), from = c(-8,8))
V(g_true)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_mu)/255)
par(mar = c(1,3,3,1), mgp = c(1.8, 0.5, 0))
plot(g_true, layout = layout_on_grid, main = "True signal", vertex.label = "")
legend_seq <- seq(-0.5, 0.5, length = 500)
for(leg_ix in 1:499){
  rect(par("usr")[1]-0.08, legend_seq[leg_ix], 
       par("usr")[1]-0.02, legend_seq[leg_ix+1],
       border = NA, col = rgb(colorRamp(col_list, bias = 1)((leg_ix-1)/500)/255),
       xpd = TRUE)
}

rect(par("usr")[1] - 0.08, -1 * 0.5, par("usr")[1]-0.02, 1 * 0.5,
     xpd = TRUE)
text(x = par("usr")[1]-0.2, y = 0.5* c(-1, -0.5, 0, 0.5, 1), labels = c(-8, -4, 0, 4, 8), xpd = TRUE)
text(x = par("usr")[1]-0.05, y = par("usr")[4] * 0.5, labels =expression(mu),
     xpd = TRUE)
```


## Experimental Setup
Now that we have $\mu_{i}$, we will generate $T = 10$ observations from each vertex.
```{r generate_data}
sigma <- 1
t <- 10
set.seed(624)

full_data <- 
  data.frame(X1 = factor(rep(0:(n-1), each = t), levels = 0:(n-1)))
full_data[,"Y"] <- rep(mu, each = t) + sigma * rnorm(n*t, mean = 0, sd = 1)

set.seed(129)
vertex_id_all <- rep(1:n, each = t)
test_vertices <- sample(1:n, size = floor(0.1 * n), replace = FALSE)
train_vertices <- (1:n)[-test_vertices]

train_index <- which(vertex_id_all %in% train_vertices)

train_data <- full_data[train_index,]

test_data <- data.frame(X1 = factor(0:(n-1), levels = 0:(n-1)))
adjacency_list <- list(X1 = A)
```

```{r create-containers}
rmse_train <- 
  c(noadj = NA, gs1 = NA, gs2 = NA, gs3 = NA, gs4 = NA, dbart = NA, bart = NA)
rmse_test <- 
  c(noadj = NA, gs1 = NA, gs2 = NA, gs3 = NA, gs4 = NA, dbart = NA, bart = NA)
timing <- 
  c(noadj = NA, gs1 = NA, gs2 = NA, gs3 = NA, gs4 = NA, dbart = NA, bart = NA)

```

To compare our results with **BART**, which cannot account for any adjacency information, we need to create a big model matrix
```{r make_X_bart}
full_model_mat <-
  dbarts::makeModelMatrixFromDataFrame(data.frame(X1 = full_data[,"X1"]))
train_model_mat <- full_model_mat[train_index,]
test_model_mat <- unique(full_model_mat)
```


We're now ready to fit our models.
In **flexBART**, we supply adjacency information using the optional argument `adjacency_list`.
This needs to be a list that contains a (named) element for every categorical predictor.
If that predictor is not network structured, the corresponding element can be `NULL`.
Otherwise, it should be an adjacency matrix whose row and column names are the levels of the variable.

We begin, however, with a version of flexBART that ignores adjacency information.
```{r fit_noadj, results='hide', cache=TRUE}
fit_noadj <-
  flexBART::flexBART(formula = Y ~ bart(X1),
           train_data = train_data,
           test_data = test_data)
rmse_train["noadj"] <-
  sqrt(mean( (fit_noadj$yhat.test.mean[train_vertices] - mu[train_vertices])^2 ))
rmse_test["noadj"] <-
  sqrt(mean( (fit_noadj$yhat.test.mean[test_vertices] - mu[test_vertices])^2 ))
timing["noadj"] <- sum(fit_noadj$timing)

```

The original flexBART paper describes four different network partitioning processes.
The process used by `flexBART::flexBART` is specified using the `graph_cut_option` argument.
```{r fit_gs1, results = 'hide', cache = TRUE}
fit_gs1 <-
  flexBART::flexBART(formula = Y ~ bart(X1),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 1)
rmse_train["gs1"] <-
  sqrt(mean( (fit_gs1$yhat.test.mean[train_vertices] - mu[train_vertices])^2 ))
rmse_test["gs1"] <-
  sqrt(mean( (fit_gs1$yhat.test.mean[test_vertices] - mu[test_vertices])^2 ))
timing["gs1"] <- sum(fit_gs1$timing)
```

```{r fit_gs2, results = 'hide', cache = TRUE}
fit_gs2 <-
  flexBART::flexBART(formula = Y ~ bart(X1),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 2)
rmse_train["gs2"] <-
  sqrt(mean( (fit_gs2$yhat.test.mean[train_vertices] - mu[train_vertices])^2 ))
rmse_test["gs2"] <-
  sqrt(mean( (fit_gs2$yhat.test.mean[test_vertices] - mu[test_vertices])^2 ))
timing["gs2"] <- sum(fit_gs2$timing)
```

```{r fit_gs3, results = 'hide', cache = TRUE}
fit_gs3 <-
  flexBART::flexBART(formula = Y ~ bart(X1),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 3)
rmse_train["gs3"] <-
  sqrt(mean( (fit_gs3$yhat.test.mean[train_vertices] - mu[train_vertices])^2 ))
rmse_test["gs3"] <-
  sqrt(mean( (fit_gs3$yhat.test.mean[test_vertices] - mu[test_vertices])^2 ))
timing["gs3"] <- sum(fit_gs3$timing)
```

```{r fit_gs4, results = 'hide', cache = TRUE}
fit_gs4 <-
  flexBART::flexBART(formula = Y ~ bart(X1),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 4)
rmse_train["gs4"] <-
  sqrt(mean( (fit_gs4$yhat.test.mean[train_vertices] - mu[train_vertices])^2 ))
rmse_test["gs4"] <-
  sqrt(mean( (fit_gs4$yhat.test.mean[test_vertices] - mu[test_vertices])^2 ))
timing["gs4"] <- sum(fit_gs4$timing)
```

```{r bart, results = 'hide', cache = TRUE}
tmp_bart <- matrix(nrow = n, ncol = 4)
tmp_time <- 0
for(cix in 1:4){
  bart_time <-
    system.time(
      bart_fit <- BART::wbart(x.train = train_data[,colnames(train_data) != "Y"],
                              y.train = train_data[,"Y"],
                              x.test = test_data[,colnames(test_data) != "Y"],
                              sparse = TRUE,
                              ntree = 50,
                              ndpost = 1000, nskip = 1000))
  tmp_bart[,cix] <- bart_fit$yhat.test.mean
  tmp_time <- tmp_time + bart_time["elapsed"]
}
tmp_bart <- rowMeans(tmp_bart)
rmse_train["bart"] <- 
  sqrt(mean( (tmp_bart[train_vertices] - mu[train_vertices])^2 ))
rmse_test["bart"] <- 
  sqrt(mean( (tmp_bart[test_vertices] - mu[test_vertices])^2 ))
timing["bart"] <- tmp_time
```

```{r dbart, results = 'hide', cache = TRUE}
tmp_dbart <- matrix(nrow = n, ncol = 4)
tmp_time <- 0
for(cix in 1:4){
  dbart_time <-
    system.time(
      dbart_fit <- dbarts::bart(x.train = train_model_mat,
                                y.train = train_data[,"Y"],
                                x.test = test_model_mat,
                                ntree = 50,
                                ndpost = 1000, nskip = 1000, keeptrees = TRUE))
  
  tmp_dbart[,cix] <- dbart_fit$yhat.test.mean
  tmp_time <- tmp_time + dbart_time["elapsed"]
}
tmp_dbart <- rowMeans(tmp_dbart)
rmse_train["dbart"] <- 
  sqrt(mean( (tmp_dbart[train_vertices] - mu[train_vertices])^2 ))
rmse_test["dbart"] <- 
  sqrt(mean( (tmp_dbart[test_vertices] - mu[test_vertices])^2 ))
timing["dbart"] <- tmp_time
```


## Results

Here are the RMSEs on the training vertices and testing vertices.
```{r rmse, results = 'hold'}
print("RMSE over training vertices:")
print(round(rmse_train, digits = 3))
print("RMSE over held-out vertices:")
print(round(rmse_test, digits = 3))
print("Time to run 4 chains:")
print(round(timing, digits = 2))
```

To get a better idea of the differences, we can plot the estimated $\mu_{i}$'s from each fitted model.
First, we create appropriate igraph objects, re-scale the fitted values to [0,1], and assign each vertex colors.

```{r make-graphs}
mu_lim <- 
  c(-1,1) * 
  max(abs(c(mu, fit_noadj$yhat.test.mean, 
            fit_gs1$yhat.test.mean, fit_gs2$yhat.test.mean,
            fit_gs3$yhat.test.mean, fit_gs4$yhat.test.mean,
            tmp_bart, tmp_dbart)))
g_heldout <- g
g_noadj <- g
g_gs1 <- g
g_gs2 <- g
g_gs3 <- g
g_gs4 <- g
g_bart <- g
g_dbart <- g

scaled_mu <- scales::rescale(mu, to = c(0,1), from = mu_lim)
scaled_noadj <-
  scales::rescale(fit_noadj$yhat.test.mean, to = c(0,1), from = mu_lim)
scaled_gs1 <- 
  scales::rescale(fit_gs1$yhat.test.mean, to = c(0,1), from = mu_lim)
scaled_gs2 <- 
  scales::rescale(fit_gs2$yhat.test.mean, to = c(0,1), from = mu_lim)
scaled_gs3 <- 
  scales::rescale(fit_gs3$yhat.test.mean, to = c(0,1), from = mu_lim)
scaled_gs4 <- 
  scales::rescale(fit_gs4$yhat.test.mean, to = c(0,1), from = mu_lim)

scaled_bart <- scales::rescale(tmp_bart, to = c(0,1), from = mu_lim)
scaled_dbart <- scales::rescale(tmp_dbart, to = c(0,1), from = mu_lim)

V(g)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_mu)/255)
V(g_heldout)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_mu)/255)
V(g_heldout)$color[test_vertices] <- my_colors[1]
V(g_noadj)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_noadj)/255)
V(g_gs1)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_gs1)/255)
V(g_gs2)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_gs2)/255)
V(g_gs3)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_gs3)/255)
V(g_gs4)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_gs4)/255)
V(g_bart)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_bart)/255)
V(g_dbart)$color <- rgb(colorRamp(col_list, bias = 1)(scaled_dbart)/255)
```


```{r plot_posterior_means, fig.align = 'center', fig.height= 6, fig.width=6}
par(mar = c(1,1,1,1), mgp = c(1.8, 0.5, 0), mfrow = c(3,3))
plot(g, layout = layout_on_grid, vertex.label = NA, main = "Truth")
plot(g_heldout, layout = layout_on_grid, vertex.label = NA, main = "Train/Test Split")
plot(g_noadj, layout = layout_on_grid, vertex.label = NA,main = "noadj")
plot(g_gs1, layout = layout_on_grid, vertex.label = NA,main = "gs1")
plot(g_gs2, layout = layout_on_grid, vertex.label = NA,main = "gs2")
plot(g_gs3, layout = layout_on_grid, vertex.label = NA,main = "gs3")
plot(g_gs4, layout = layout_on_grid, vertex.label = NA,main = "gs4")
plot(g_bart, layout = layout_on_grid, vertex.label = NA,main = "bart")
plot(g_dbart, layout = layout_on_grid, vertex.label = NA,main = "dbart")

```

We see that on the training vertices, BART can get pretty accurate estimates of $\mu.$
However, because BART does not know how to leverage the adjacency information, it is forced to make the same prediction at each of the heldout vertices.
Specifically, at the heldout vertices, BART predicts $\mu_{i}$ to be about equal to the grand mean of the data $\overline{y}.$

In contrast, **flexBART** is able to leverage the adjacency information and make much more accurate predictions at the heldout vertices.