---
title: "network_linked_regression"
output: rmarkdown::github_document
date: '2025-05-08'
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

## Overview
In this example, we show how to use `flexBART::flexBART` for network-linked regression.
At each vertex $v$ of a given network $\mathcal{G},$ we observe $n_{v}$ covariate-response pairs $(\boldsymbol{x}_{vt}, y_{vt})$ of a $p$-dimensional covariate vector $\boldsymbol{x}_{vt} \in [-1,1]^{p}$ and a scalar response $y.$
  We further model
$$
  y_{vt} \sim \mathcal{N}(f(\mathbf{x}_{vt},v), \sigma^{2})
$$
where $f: [-1,1]^{p} \times V(\mathcal{G}) \rightarrow \mathbb{R}$ is an unknown regression function.
Implicit in our notation is the assumption that the regression relationship is not necessarily the same at every vertex.


One option to learning $f$ is to fit separate BART models to the observations at each vertex (i.e. use the $(\boldsymbol{x}_{vt}, y_{vt})$ pairs to estimate the function $f(\boldsymbol{x}, v)$ for each $v$).
Because it models the data at each vertex completely independently, such a strategy is unable to exploit network smoothness, in the sense that for two adjacent vertices $v \sim v',$ we might have $f(\boldsymbol{x}, v) \approx f(\boldsymbol{x},v').$
In the presence of such smoothness, it may be desirable to "partially pool" the observations across adjacent vertices.
We now demonstrate how `flexBART::flexBART` can perform such partial pooling using the available adjacency information.

To this end, we will simulate data on a real network that encodes the spatial adjacency of a subset of the 2010 Census tracts in the city of Philadelphia.
The file `network_linked_regression_data.RData` contains an **igraph** object containing this network, a nice layout matrix for plotting, and the adjacency matrix for this network. 
The row and column names of this matrix correspond to the vertex labels of the network.

```{r load_network}
library(igraph)
load("network_linked_regression_data.RData")
col_list <- colorBlindness::Blue2DarkRed18Steps
my_colors <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

We can visualize the network using a couple of different layouts
```{r visualize_network}
n <- nrow(A) # number of vertices
cols <- rep(my_colors[1], times = n)
V(g)$color <- cols

par(mar = c(1,1,1,1), mgp = c(1.8, 0.5, 0), mfrow = c(1,3))


plot(g, layout = layout_nicely, vertex.size = 3.5, vertex.label.size = 2.5, main = "layout_nicely()")
plot(g, layout = layout_in_circle, vertex.size = 3.5, vertex.label.size = 2.5, main = "layout_in_circle()")
plot(g, layout = my_layout, vertex.size = 3.5, vertex.label.size = 2.5, main = "Our layout")
```

## Data Generation

We will generate $n_{v} = 100$ observations at each vertex using the following steps

  1. Create two connected clusters (`cluster1` and `cluster2`), each containing a small number of vertices.
  2. For all vertices in the network, compute the shortest path distances between that vertex and each of `cl1` and `cl2`. Denote these distances $d_{i1}$ and $d_{i2}.$
  3. Define the weight of vertex $v$ as $w_{v} = d_{i2}/(d_{i1} + d_{i2}).$ Note that if $v$ is in `cl1`, $w_{v} = 0$ and if $v$ is in `cl2`, $w_{v} = 1.$ Vertices with weight larger than 0.5 are, in some sense, closer to `cl2` than they are to `cl1.`
  4. Draw independent covariate vectors $\boldsymbol{x}_{v1}, \ldots, \boldsymbol{x}_{v250} \sim \mathcal{U}[-1,1]^{10}$ at each vertex $v$ and generate $y_{vt} \sim \mathcal{N}(f(\boldsymbol{x}_{vt}), 1)$ where the true regression function $f$ is convex combination of two base functions
  
\begin{align}
f(\boldsymol{x}, v) &= w_{v}f_{1}(\boldsymbol{x}) + (1 - w_{v})f_{2}(\boldsymbol{x}) \\
f_{1}(\boldsymbol{x}) &+ 3\tilde{x}_{1} + (2 - 5 \times \tilde{x}_{2} \times \sin(\pi \tilde{x}_{1}) - 2 \times \tilde{x}_{2}\\
f_{2}(\boldsymbol{x}) &= (3 - 3\times \cos(6\pi \tilde{x}_{1})) \times \boldsymbol{1}(\tilde{x}_{1} > 0.6) - (10\sqrt{\tilde{x}_{1}}) \times \boldsymbol{1}(\tilde{x}_{1} < 0.25),
\end{align}
where $\tilde{x}_{1} = (1 + x_{1})/2$ and $\tilde{x}_{2} = \boldsymbol{1}(X_{2} > 0).$

Here we form the clusters, compute the vertex weights, and then visualize the weights of each vertex
```{r form_clusters}
cluster1 <- c(35, 37, 36, 40, 48, 44, 54, 67, 59)
cluster2 <- c(20, 2, 18, 17, 21, 15, 16)
cluster3 <- (1:n)[!(1:n) %in% c(cluster1, cluster2)]
dist_to_cl1 <- apply(igraph::distances(graph = g, to = cluster1),
                     FUN = min, MARGIN = 1)
dist_to_cl2 <- apply(igraph::distances(graph = g, to = cluster2),
                     FUN = min, MARGIN = 1)
total_dist <- dist_to_cl1 + dist_to_cl2
w <- dist_to_cl2/total_dist # how much weight to give to f1
g_weight <- g
V(g_weight)$color <- rgb(colorRamp(col_list, bias = 1)(w)/255)
par(mar = c(1,1,1,1), mgp = c(1.8, 0.5, 0))
plot(g_weight, layout = my_layout, vertex.label = NA,
     vertex.size = 3.5)
```

We can now define the base functions $f_{1}$ and $f_{2}$ and plot them.
```{r define-base-functions}
f1_true <- function(X_cont){
  scaled_X_cont <- (X_cont + 1)/2 # moves it to [0,1]
  z1 <- scaled_X_cont[,1]
  z2 <- 1*(scaled_X_cont[,2] > 0.5)
  return(3 * z1 + (2 - 5 * z2) * sin(pi * z1) - 2 * z2)
}
f2_true <- function(X_cont){
  scaled_X_cont <- (X_cont + 1)/2
  return( (3 - 3*cos(6*pi*scaled_X_cont[,1]) * scaled_X_cont[,1]^2) * (scaled_X_cont[,1] > 0.6) - (10 * sqrt(scaled_X_cont[,1])) * (scaled_X_cont[,1] < 0.25) )
}
mu_true <- function(X_cont, vertex_id){
  tmp1 <- f1_true(X_cont)
  tmp2 <- f2_true(X_cont)
  tmp_w <- w[vertex_id]
  return(tmp_w * tmp1 + (1 - tmp_w)*tmp2)
}
```
                                           
```{r plot-functions}
x1_seq <- seq(-1,1, length = 1001)
x_plot <- cbind(c(x1_seq, x1_seq), c(rep(-0.5, times = 1001), rep(0.5, times = 1001)))
f1_plot <- f1_true(x_plot)
par(mar = c(3,3,2,1), mgp = c(1.8, 0.5, 0), mfrow = c(1,2))
plot(x1_seq, f1_plot[1:1001], type = "l", xlim = c(-1,1), ylim = range(f1_plot),
     xlab = expression(x[1]), ylab = "", main = "Base function 1")
lines( x1_seq, f1_plot[1002:2002], lty = 2)
legend("bottomright", legend = expression(x[2] > 0), lty = 2, bty = "n", cex = 1.1)
legend("topleft", legend = expression(x[2] <= 0), lty = 1, bty = "n", cex = 1.1)

f2_plot <- f2_true(x_plot[1:1001,])
z1_seq <- (1 + x1_seq)/2
plot(1, type = "n", xlim = c(-1,1), ylim = range(f2_plot),
     xlab = expression(x[1]), ylab = "", main = "Base function 2")
lines(x1_seq[z1_seq < 0.25], f2_plot[z1_seq < 0.25])
lines(x1_seq[z1_seq >= 0.25 & z1_seq < 0.6], f2_plot[z1_seq >= 0.25 & z1_seq < 0.6])
lines(x1_seq[z1_seq > 0.6], f2_plot[z1_seq > 0.6])
```
                                           
## Experimental Setup
Now that we have defined the functions, we will generate $n_{v} = 250$ observations at each vertex.

```{r generate_all_data}
set.seed(508)
n_v <- 200
vertex_id_all <- rep(1:n, each = n_v)
p_cont <- 10
p_cat <- 1
full_data <- data.frame(Y = rep(NA, times = n*n_v))
for(j in 1:p_cont) full_data[,paste0("X",j)] <- runif(n = n*n_v, min = -1, max = 1)
full_data[,"vertex"] <- factor(vertex_id_all, levels = 1:n)
mu_all <- mu_true(full_data[,paste0("X",1:p_cont)], vertex_id_all)
sigma <- 1
full_data[,"Y"] <- mu_all + sigma * rnorm(n = n*n_v, mean = 0, sd = 1)
adjacency_list <- list(vertex = A)

#model matrix needed for dbarts
full_mm <- 
  dbarts::makeModelMatrixFromDataFrame(full_data[,colnames(full_data) != "Y"])
```

To make things interesting, we will train our model using data from only 90\% of all vertices.
We will interested in understanding how well we can recover $f$ when we (i) have observed data at vertex $i$ and (ii) when we have not observed data at vertex $i$. 
Note: when training our model, we will use the full adjacency information.
This way we can assess how well our model leverages the adjacency structure to predict $f$ at the held-out vertices.
To evaluate how we well can estimate $f(\boldsymbol{x},v)$ out-of-sample, we generate $50$ more observations at each vertex.
We will evaluate performance using two sets of vertices, those which appeared in the training dataset and those which did not.
```{r create-train-test-split}
test_vertices <- sample(1:n, size = floor(0.1 * n), replace = FALSE)
train_vertices <- (1:n)[-test_vertices]
train_index <- which(vertex_id_all %in% train_vertices)
train_data <- full_data[train_index,]
mu_train <- mu_all[train_index]
train_mm <- full_mm[train_index,]

n_test <- 100
vertex_id_test <- rep(1:n, each = n_test)
test_data <- data.frame(X1 = runif(n=n_test*n, min = -1, max = 1))
for(j in 2:p_cont) test_data[,paste0("X",j)] <- runif(n=n_test*n, min = -1, max = 1)
test_data[,"vertex"] <- factor(vertex_id_test, levels = 1:n)
test_mm <- 
  dbarts::makeModelMatrixFromDataFrame(test_data)
mu_test <- mu_true(test_data[,paste0("X",1:p_cont)], vertex_id_test)

test_index_1 <- which(vertex_id_test %in% train_vertices) # test obs for vertices in training
test_index_2 <- which(vertex_id_test %in% test_vertices) # test obs for vertices not in training
```

Here is a plot of the training (gray) and testing vertices (yellow).
```{r plot-training-testing}
g_train <- g
training_cols <- rep(my_colors[1], times = n)
training_cols[test_vertices] <- my_colors[5]
V(g_train)$color <- training_cols
plot(g_train, layout = my_layout, vertex.size = 3.5, vertex.label = NA)
```

```{r create-containers}
rmse <- matrix(NA, nrow = 7, ncol = 3,
               dimnames = list(c("noadj", "gs1", "gs2", "gs3", "gs4", "bart", "dbart"), 
                               c("Train", "Test1", "Test2")))
coverage <- rmse
timing <- 
  c(noadj = NA, gs1 = NA, gs2 = NA, gs3 = NA, gs4 = NA, bart = NA, dbart = NA)
```

## Model fitting

```{r fit-no-adj, cache=TRUE, results='hide'}
fit <-
  flexBART::flexBART(formula = Y ~ bart(.),
           train_data = train_data,
           test_data = test_data)
timing["noadj"] <- sum(fit$timing)
rmse["noadj", "Train"] <-
  sqrt(mean( (mu_train - fit$yhat.train.mean)^2 ))
rmse["noadj", "Test1"] <- 
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))
rmse["noadj", "Test2"] <-
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))

l95_train <-
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_train <- 
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.975)

l95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.975)

coverage["noadj", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["noadj", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["noadj", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])
timing["noadj"] <- sum(fit$timing)
rm(fit, l95_train, u95_train, l95_test, u95_test)
```

```{r fit-gs1, cache=TRUE, results='hide'}
fit <-
  flexBART::flexBART(formula = Y ~ bart(.),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 1)
rmse["gs1", "Train"] <-
  sqrt(mean( (mu_train - fit$yhat.train.mean)^2 ))
rmse["gs1", "Test1"] <- 
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))
rmse["gs1", "Test2"] <-
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))

l95_train <-
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_train <- 
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.975)

l95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.975)

coverage["gs1", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["gs1", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["gs1", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["gs1"] <- sum(fit$timing)
rm(fit, l95_train, u95_train, l95_test, u95_test)
```


```{r fit-gs2, cache=TRUE, results='hide'}
fit <-
  flexBART::flexBART(formula = Y ~ bart(.),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 2)
rmse["gs2", "Train"] <-
  sqrt(mean( (mu_train - fit$yhat.train.mean)^2 ))
rmse["gs2", "Test1"] <- 
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))
rmse["gs2", "Test2"] <-
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))

l95_train <-
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_train <- 
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.975)

l95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.975)

coverage["gs2", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["gs2", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["gs2", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["gs2"] <- sum(fit$timing)
rm(fit, l95_train, u95_train, l95_test, u95_test)
```


```{r fit-gs3, cache=TRUE, results='hide'}
fit <-
  flexBART::flexBART(formula = Y ~ bart(.),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 3)
rmse["gs3", "Train"] <-
  sqrt(mean( (mu_train - fit$yhat.train.mean)^2 ))
rmse["gs3", "Test1"] <- 
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))
rmse["gs3", "Test2"] <-
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))

l95_train <-
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_train <- 
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.975)

l95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.975)

coverage["gs3", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["gs3", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["gs3", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["gs3"] <- sum(fit$timing)
rm(fit, l95_train, u95_train, l95_test, u95_test)
```


```{r fit-gs4, cache=TRUE, results='hide'}
fit <-
  flexBART::flexBART(formula = Y ~ bart(.),
           train_data = train_data,
           test_data = test_data,
           inform_sigma = TRUE,
           adjacency_list = adjacency_list,
           graph_cut_type = 4)
rmse["gs4", "Train"] <-
  sqrt(mean( (mu_train - fit$yhat.train.mean)^2 ))
rmse["gs4", "Test1"] <- 
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))
rmse["gs4", "Test2"] <-
  sqrt(mean((mu_test[test_index_1] - fit$yhat.test.mean[test_index_1])^2 ))

l95_train <-
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_train <- 
  apply(fit$yhat.train, MARGIN = 2, FUN = quantile, probs = 0.975)

l95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(fit$yhat.test, MARGIN = 2, FUN = quantile, probs = 0.975)

coverage["gs4", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["gs4", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["gs4", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["gs4"] <- sum(fit$timing)
rm(fit, l95_train, u95_train, l95_test, u95_test)
```



```{r fit-bart, cache=TRUE, results='hide'}
bart_train <- list()
bart_test <- list()
bart_time <- rep(NA, times = 4)
for(c_ix in 1:4){
  tmp_time <-
    system.time(
      tmp_fit <-
        BART::wbart(x.train = train_mm,
                    y.train = train_data[,"Y"],
                    x.test = test_mm,
                    ntree = 50, sparse = TRUE,
                    ndpost = 1000, nskip = 1000))
  bart_train[[c_ix]] <- tmp_fit$yhat.train
  bart_test[[c_ix]] <- tmp_fit$yhat.test
  bart_time[c_ix] <- tmp_time["elapsed"]
}
train_samples <-
  do.call(rbind, bart_train)
test_samples <-
  do.call(rbind, bart_test)
rm(bart_train, bart_test)

train_mean <- 
  apply(train_samples, MAR = 2, FUN = mean)
l95_train <-
  apply(train_samples, MAR = 2, FUN = quantile, probs = 0.025)
u95_train <-
  apply(train_samples, MAR = 2, FUN = quantile, probs = 0.975)
test_mean <- 
  apply(test_samples, MAR = 2, FUN = mean)
l95_test <-
  apply(test_samples, MAR = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(test_samples, MAR = 2, FUN = quantile, probs = 0.975)

rmse["bart", "Train"] <-
  sqrt(mean( (mu_train - train_mean)^2 ))
rmse["bart", "Test1"] <-
  sqrt(mean( (mu_test[test_index_1] - test_mean[test_index_1])^2 ))
rmse["bart", "Test2"] <-
  sqrt(mean( (mu_test[test_index_2] - test_mean[test_index_2])^2 ))

coverage["bart", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["bart", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["bart", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["bart"] <- sum(bart_time)
rm(train_mean, test_mean, l95_train, u95_train, l95_test, u95_test)
```


```{r fit-dbart, cache=TRUE, results='hide'}
dbart_train <- list()
dbart_test <- list()
dbart_time <- rep(NA, times = 4)

for(c_ix in 1:4){
  tmp_time <-
    system.time(
      tmp_fit <-
        dbarts::bart(x.train = train_mm,
                    y.train = train_data[,"Y"],
                    x.test = test_mm,
                    ntree = 50, 
                    ndpost = 1000, nskip = 1000))
  dbart_train[[c_ix]] <- tmp_fit$yhat.train
  dbart_test[[c_ix]] <- tmp_fit$yhat.test
  dbart_time[c_ix] <- tmp_time["elapsed"]
}
train_samples <-
  do.call(rbind, dbart_train)
test_samples <-
  do.call(rbind, dbart_test)
rm(dbart_train, dbart_test)

train_mean <- 
  apply(train_samples, MAR = 2, FUN = mean)
l95_train <-
  apply(train_samples, MAR = 2, FUN = quantile, probs = 0.025)
u95_train <-
  apply(train_samples, MAR = 2, FUN = quantile, probs = 0.975)
test_mean <- 
  apply(test_samples, MAR = 2, FUN = mean)
l95_test <-
  apply(test_samples, MAR = 2, FUN = quantile, probs = 0.025)
u95_test <-
  apply(test_samples, MAR = 2, FUN = quantile, probs = 0.975)

rmse["dbart", "Train"] <-
  sqrt(mean( (mu_train - train_mean)^2 ))
rmse["dbart", "Test1"] <-
  sqrt(mean( (mu_test[test_index_1] - test_mean[test_index_1])^2 ))
rmse["dbart", "Test2"] <-
  sqrt(mean( (mu_test[test_index_2] - test_mean[test_index_2])^2 ))

coverage["dbart", "Train"] <-
  mean(mu_train >= l95_train & 
         mu_train <= u95_train)
coverage["dbart", "Test1"] <-
  mean(mu_test[test_index_1] >= l95_test[test_index_1] & 
         mu_test[test_index_1] <= u95_test[test_index_1])
coverage["dbart", "Test2"] <-
  mean(mu_test[test_index_2] >= l95_test[test_index_2] & 
         mu_test[test_index_2] <= u95_test[test_index_2])

timing["dbart"] <- sum(dbart_time)
rm(l95_train, u95_train, l95_test, u95_test)
```

## Results

Here are the RMSE, uncertainty interval coverage, and timing results. 
```{r results, results='hold'}
print(round(rmse, digits = 3))
print(round(coverage, digits = 3))
print(round(timing, digits = 3))
```
